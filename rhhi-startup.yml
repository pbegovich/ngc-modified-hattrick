- name: buildrhhi
  hosts: 172.16.30.23
  gather_facts: false

  pre_tasks:

    - name: start nodes
      shell: ipmitool -I lanplus -H {{ item }} -U admin -P admin chassis power on
      with_items:
        - "{{ ipmi2 }}"
        - "{{ ipmi3 }}"
        - "{{ ipmi4 }}"
      delegate_to: localhost

  vars:
    vms: 
      - "ocp-appnode-1-7dfa"
      - "ocp-appnode-2-7dfa"
      - "ocp-appnode-3-7dfa" 
      - "ocp-infranode-1-c49a"
      - "ocp-master-1-01f8"
      - "ocp-nfs-1-5bf0"
      - "rhvsuite"
      - "satellite-1-35d0"
      - "tower-ldo-1-8a4c"

  tasks:
    - name: check connection
      wait_for_connection:

    - name: check peers count
      shell: "gluster peer status | head -1 | awk '{ print $4 }'"
      register: peer_count

    - name: fail when peer count not 2
      fail:
        msg: 'peer count not 2'
      when: peer_count.stdout != "2"

    - name: check if first host is in connected state
      shell: "gluster peer status | awk NR==5 | awk '{ print $5 }' | tr -d '\n'"
      register: first_host

    - name: debug first_host
      debug: msg="{{ first_host.stdout }}"

    - name: check if first gluster host is connected
      fail:
        msg: 'First Gluster has peers in disconnected state'
      when: first_host.stdout != "(Connected)"


    - name: check if second host is in connected state
      shell: "gluster peer status | awk NR==9 | awk '{ print $5 }' | tr -d '\n'"
      register: second_host

    - name: debug second_host
      debug: msg="{{ second_host.stdout }}"

    - name: check if second gluster host is connected
      fail:
        msg: 'Second Gluster has peers in disconnected state'
      when: second_host.stdout != "(Connected)"


    - name: pause for storage
      pause:
        seconds: 30

    - name: attach storage
      shell: hosted-engine --connect-storage

    - name: wait for ovirt-ha to kick in
      shell: hosted-engine --vm-status
      register: hosted
      retries: 100
      delay: 10
      until: hosted.stdout.find("ovirt-ha-agent") == -1

    - name: move global maintenance mode to none
      shell: hosted-engine --set-maintenance --mode=none

    - name: check for hosted engine to be up
      shell: hosted-engine --vm-status
      register: "vm_up"
      retries: 100
      until: vm_up.stdout.find("good") != -1


    - name: pause 100
      pause:
        seconds: 100

    - name: Authenticate to RHV-M
      ovirt_auth:
       url: "https://{{ rhvurl }}/ovirt-engine/api"
       username: "admin@internal"
       password: "{{ rhv_pass }}"
       insecure: true

    - name: active data domain
      ovirt_storage_domains:
        auth: "{{ ovirt_auth }}"
        name: data
        host: "{{ rhv4_mpc_hostname }}"
        data_center: Default
        domain_function: data
        wait: true
        state: present
        glusterfs:
          address: "{{ mpc_rhv4_ip }}"
          path: /data
          mount_options: backup-volfile-servers={{ mpc_rhv2_ip }}:{{ mpc_rhv3_ip }}

    - name: active vmstore domain
      ovirt_storage_domains:
        auth: "{{ ovirt_auth }}"
        name: vmstore
        host: "{{ rhv4_mpc_hostname }}"
        data_center: Default
        domain_function: data
        wait: true
        state: present
        glusterfs:
          address: "{{ mpc_rhv4_ip }}"
          path: /vmstore
          mount_options: backup-volfile-servers={{ mpc_rhv2_ip }}:{{ mpc_rhv3_ip }}

    - name: activate iso domain
      ovirt_storage_domains:
        auth: "{{ ovirt_auth }}"
        name: iso
        host: "{{ rhv4_mpc_hostname }}"
        domain_function: iso
        data_center: Default
        wait: true
        state: present
        glusterfs:
          address: "{{ mpc_rhv4_ip }}"
          path: /iso
          mount_options: backup-volfile-servers={{ mpc_rhv2_ip }}:{{ mpc_rhv3_ip }}

    - name: pause 100
      pause:
        seconds: 100
 
    - name: start vms
      ovirt_vms:
        auth: "{{ ovirt_auth }}"
        state: "running"
        wait: true
        name: "{{ item }}"
      with_items: "{{ vms }}"  
        
