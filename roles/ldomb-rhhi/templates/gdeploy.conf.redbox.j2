#gdeploy configuration generated by cockpit-gluster plugin
[hosts]
{{ mpc_rhv4_ip }}
{{ mpc_rhv3_ip }}
{{ mpc_rhv2_ip }}

[script1:{{ mpc_rhv4_ip }}]
action=execute
ignore_script_errors=no
file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -d sda,sdb,sdc,sdd -h {{ mpc_rhv4_ip }},{{ mpc_rhv3_ip }},{{ mpc_rhv2_ip }}

[script1:{{ mpc_rhv3_ip }}]
action=execute
ignore_script_errors=no
file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -d sda,sdb,sdc,sdd -h {{ mpc_rhv4_ip }},{{ mpc_rhv3_ip }},{{ mpc_rhv2_ip }}

[script1:{{ mpc_rhv2_ip }}]
action=execute
ignore_script_errors=no
file=/usr/share/gdeploy/scripts/grafton-sanity-check.sh -d sda,sdb,sdc,sdd -h {{ mpc_rhv4_ip }},{{ mpc_rhv3_ip }},{{ mpc_rhv2_ip }}

[disktype]
jbod

[diskcount]
12

[stripesize]
256

[service1]
action=enable
service=chronyd

[service2]
action=restart
service=chronyd

[shell2]
action=execute
command=vdsm-tool configure --force

[script3]
action=execute
file=/usr/share/gdeploy/scripts/blacklist_all_disks.sh
ignore_script_errors=no

[pv1:{{ mpc_rhv4_ip }}]
action=create
devices=sda
ignore_pv_errors=no

[pv2:{{ mpc_rhv4_ip }}]
action=create
devices=sdb
ignore_pv_errors=no

[pv3:{{ mpc_rhv4_ip }}]
action=create
devices=sdc
ignore_pv_errors=no

[pv4:{{ mpc_rhv4_ip }}]
action=create
devices=sdd
ignore_pv_errors=no

[pv1:{{ mpc_rhv3_ip }}]
action=create
devices=sda
ignore_pv_errors=no

[pv2:{{ mpc_rhv3_ip }}]
action=create
devices=sdb
ignore_pv_errors=no

[pv3:{{ mpc_rhv3_ip }}]
action=create
devices=sdc
ignore_pv_errors=no

[pv4:{{ mpc_rhv3_ip }}]
action=create
devices=sdd
ignore_pv_errors=no

[pv1:{{ mpc_rhv2_ip }}]
action=create
devices=sda
ignore_pv_errors=no

[pv2:{{ mpc_rhv2_ip }}]
action=create
devices=sdb
ignore_pv_errors=no

[pv3:{{ mpc_rhv2_ip }}]
action=create
devices=sdc
ignore_pv_errors=no

[pv4:{{ mpc_rhv2_ip }}]
action=create
devices=sdd
ignore_pv_errors=no

[vg1:{{ mpc_rhv4_ip }}]
action=create
vgname=gluster_vg_sda
pvname=sda
ignore_vg_errors=no

[vg2:{{ mpc_rhv4_ip }}]
action=create
vgname=gluster_vg_sdb
pvname=sdb
ignore_vg_errors=no

[vg3:{{ mpc_rhv4_ip }}]
action=create
vgname=gluster_vg_sdc
pvname=sdc
ignore_vg_errors=no

[vg4:{{ mpc_rhv4_ip }}]
action=create
vgname=gluster_vg_sdd
pvname=sdd
ignore_vg_errors=no

[vg1:{{ mpc_rhv3_ip }}]
action=create
vgname=gluster_vg_sda
pvname=sda
ignore_vg_errors=no

[vg2:{{ mpc_rhv3_ip }}]
action=create
vgname=gluster_vg_sdb
pvname=sdb
ignore_vg_errors=no

[vg3:{{ mpc_rhv3_ip }}]
action=create
vgname=gluster_vg_sdc
pvname=sdc
ignore_vg_errors=no

[vg4:{{ mpc_rhv3_ip }}]
action=create
vgname=gluster_vg_sdd
pvname=sdd
ignore_vg_errors=no

[vg1:{{ mpc_rhv2_ip }}]
action=create
vgname=gluster_vg_sda
pvname=sda
ignore_vg_errors=no

[vg2:{{ mpc_rhv2_ip }}]
action=create
vgname=gluster_vg_sdb
pvname=sdb
ignore_vg_errors=no

[vg3:{{ mpc_rhv2_ip }}]
action=create
vgname=gluster_vg_sdc
pvname=sdc
ignore_vg_errors=no

[vg4:{{ mpc_rhv2_ip }}]
action=create
vgname=gluster_vg_sdd
pvname=sdd
ignore_vg_errors=no

[lv1:{{ mpc_rhv4_ip }}]
action=create
poolname=gluster_thinpool_sdb
ignore_lv_errors=no
vgname=gluster_vg_sdb
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv2:{{ mpc_rhv4_ip }}]
action=create
poolname=gluster_thinpool_sdc
ignore_lv_errors=no
vgname=gluster_vg_sdc
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv3:{{ mpc_rhv4_ip }}]
action=create
poolname=gluster_thinpool_sdd
ignore_lv_errors=no
vgname=gluster_vg_sdd
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv4:{{ mpc_rhv3_ip }}]
action=create
poolname=gluster_thinpool_sdb
ignore_lv_errors=no
vgname=gluster_vg_sdb
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv5:{{ mpc_rhv3_ip }}]
action=create
poolname=gluster_thinpool_sdc
ignore_lv_errors=no
vgname=gluster_vg_sdc
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv6:{{ mpc_rhv3_ip }}]
action=create
poolname=gluster_thinpool_sdd
ignore_lv_errors=no
vgname=gluster_vg_sdd
lvtype=thinpool
size=800GB
poolmetadatasize=4GB

[lv7:{{ mpc_rhv2_ip }}]
action=create
poolname=gluster_thinpool_sdb
ignore_lv_errors=no
vgname=gluster_vg_sdb
lvtype=thinpool
size=20GB
poolmetadatasize=1GB

[lv8:{{ mpc_rhv2_ip }}]
action=create
poolname=gluster_thinpool_sdc
ignore_lv_errors=no
vgname=gluster_vg_sdc
lvtype=thinpool
size=20GB
poolmetadatasize=1GB

[lv9:{{ mpc_rhv2_ip }}]
action=create
poolname=gluster_thinpool_sdd
ignore_lv_errors=no
vgname=gluster_vg_sdd
lvtype=thinpool
size=20GB
poolmetadatasize=1GB

[lv10:{{ mpc_rhv4_ip }}]
action=create
lvname=gluster_lv_engine
ignore_lv_errors=no
vgname=gluster_vg_sda
mount=/gluster_bricks/engine
size=100GB
lvtype=thick

[lv11:{{ mpc_rhv4_ip }}]
action=create
lvname=gluster_lv_data
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/data
lvtype=thinlv
poolname=gluster_thinpool_sdb
virtualsize=800GB

[lv12:{{ mpc_rhv4_ip }}]
action=create
lvname=gluster_lv_vmstore
ignore_lv_errors=no
vgname=gluster_vg_sdc
mount=/gluster_bricks/vmstore
lvtype=thinlv
poolname=gluster_thinpool_sdc
virtualsize=800GB

[lv13:{{ mpc_rhv4_ip }}]
action=create
lvname=gluster_lv_iso
ignore_lv_errors=no
vgname=gluster_vg_sdd
mount=/gluster_bricks/iso
lvtype=thinlv
poolname=gluster_thinpool_sdd
virtualsize=800GB

[lv14:{{ mpc_rhv3_ip }}]
action=create
lvname=gluster_lv_engine
ignore_lv_errors=no
vgname=gluster_vg_sda
mount=/gluster_bricks/engine
size=100GB
lvtype=thick

[lv15:{{ mpc_rhv3_ip }}]
action=create
lvname=gluster_lv_data
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/data
lvtype=thinlv
poolname=gluster_thinpool_sdb
virtualsize=800GB

[lv16:{{ mpc_rhv3_ip }}]
action=create
lvname=gluster_lv_vmstore
ignore_lv_errors=no
vgname=gluster_vg_sdc
mount=/gluster_bricks/vmstore
lvtype=thinlv
poolname=gluster_thinpool_sdc
virtualsize=800GB

[lv17:{{ mpc_rhv3_ip }}]
action=create
lvname=gluster_lv_iso
ignore_lv_errors=no
vgname=gluster_vg_sdd
mount=/gluster_bricks/iso
lvtype=thinlv
poolname=gluster_thinpool_sdd
virtualsize=800GB

[lv18:{{ mpc_rhv2_ip }}]
action=create
lvname=gluster_lv_engine
ignore_lv_errors=no
vgname=gluster_vg_sda
mount=/gluster_bricks/engine
size=20GB
lvtype=thick

[lv19:{{ mpc_rhv2_ip }}]
action=create
lvname=gluster_lv_data
ignore_lv_errors=no
vgname=gluster_vg_sdb
mount=/gluster_bricks/data
lvtype=thinlv
poolname=gluster_thinpool_sdb
virtualsize=20GB

[lv20:{{ mpc_rhv2_ip }}]
action=create
lvname=gluster_lv_vmstore
ignore_lv_errors=no
vgname=gluster_vg_sdc
mount=/gluster_bricks/vmstore
lvtype=thinlv
poolname=gluster_thinpool_sdc
virtualsize=20GB

[lv21:{{ mpc_rhv2_ip }}]
action=create
lvname=gluster_lv_iso
ignore_lv_errors=no
vgname=gluster_vg_sdd
mount=/gluster_bricks/iso
lvtype=thinlv
poolname=gluster_thinpool_sdd
virtualsize=20GB

[selinux]
yes

[service3]
action=restart
service=glusterd
slice_setup=yes

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp
services=glusterfs

[script2]
action=execute
file=/usr/share/gdeploy/scripts/disable-gluster-hooks.sh

[shell3]
action=execute
command=usermod -a -G gluster qemu

[volume1]
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal
value=virt,36,36,30,on,off,enable
brick_dirs={{ mpc_rhv4_ip }}:/gluster_bricks/engine/engine,{{ mpc_rhv3_ip }}:/gluster_bricks/engine/engine,{{ mpc_rhv2_ip }}:/gluster_bricks/engine/engine
ignore_volume_errors=no
arbiter_count=1

[volume2]
action=create
volname=data
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal
value=virt,36,36,30,on,off,enable
brick_dirs={{ mpc_rhv4_ip }}:/gluster_bricks/data/data,{{ mpc_rhv3_ip }}:/gluster_bricks/data/data,{{ mpc_rhv2_ip }}:/gluster_bricks/data/data
ignore_volume_errors=no
arbiter_count=1

[volume3]
action=create
volname=vmstore
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal
value=virt,36,36,30,on,off,enable
brick_dirs={{ mpc_rhv4_ip }}:/gluster_bricks/vmstore/vmstore,{{ mpc_rhv3_ip }}:/gluster_bricks/vmstore/vmstore,{{ mpc_rhv2_ip }}:/gluster_bricks/vmstore/vmstore
ignore_volume_errors=no
arbiter_count=1

[volume4]
action=create
volname=iso
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal
value=virt,36,36,30,on,off,enable
brick_dirs={{ mpc_rhv4_ip }}:/gluster_bricks/iso/iso,{{ mpc_rhv3_ip }}:/gluster_bricks/iso/iso,{{ mpc_rhv2_ip }}:/gluster_bricks/iso/iso
ignore_volume_errors=no
arbiter_count=1

# Disable multipath
[script3]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-multipath.sh
